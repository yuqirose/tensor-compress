{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "# %load unit_test.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import embedding_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "\n",
    "import tensornet\n",
    "from tensorflow.python.ops.rnn_cell import *\n",
    "\n",
    "\n",
    "class TensorBasicLSTMCell(LSTMCell):\n",
    "    \"\"\"Tensor Factorized Long short-term memory unit (LSTM) recurrent network cell.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_units, **kwargs):\n",
    "        super(TensorBasicLSTMCell, self).__init__(num_units)\n",
    "#         self._inp_modes = kwargs['inp_modes']\n",
    "#         self._out_modes = kwargs['out_modes']\n",
    "        self._mat_ranks = kwargs['mat_ranks']\n",
    "            \n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "        with vs.variable_scope(scope or type(self).__name__):  # \"BasicLSTMCell\"\n",
    "            # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "            if self._state_is_tuple:\n",
    "                c, h = state\n",
    "            else:\n",
    "                c, h = array_ops.split(1, 2, state)     \n",
    "        \n",
    "            i = linear_tt([inputs, h], self._num_units, self._mat_ranks, bias =True, scope = \"i\")  \n",
    "            j = linear_tt([inputs, h], self._num_units, self._mat_ranks, bias =True, scope = \"j\")   \n",
    "            f = linear_tt([inputs, h], self._num_units, self._mat_ranks, bias =True, scope = \"f\")   \n",
    "            o = linear_tt([inputs, h], self._num_units, self._mat_ranks, bias =True, scope = \"o\")   \n",
    "        \n",
    "#             concat = _linear([inputs, h], 4 * self._num_units, True)\n",
    "#             # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "#             i , j, f, o = array_ops.split(1, 4, concat)\n",
    "\n",
    "            new_c = (c * sigmoid(f + self._forget_bias) + sigmoid(i) *\n",
    "                     self._activation(j))\n",
    "            new_h = self._activation(new_c) * sigmoid(o)\n",
    "\n",
    "            if self._state_is_tuple:\n",
    "                new_state = LSTMStateTuple(new_c, new_h)\n",
    "            else:\n",
    "                new_state = array_ops.concat(1, [new_c, new_h])\n",
    "            return new_h, new_state\n",
    "\n",
    "def linear_tt(args, output_size,  mat_ranks, bias, bias_start=0.0, scope=None):\n",
    "    \"\"\"wrapper for factorization layer\"\"\"\n",
    "    # args = [x, h] solve y = Wx + Uh + b\n",
    "    if args is None or (nest.is_sequence(args) and not args):\n",
    "        raise ValueError(\"`args` must be specified\")\n",
    "    if not nest.is_sequence(args):\n",
    "        args = [args]\n",
    "\n",
    "    # Calculate the total size of arguments on dimension 1.\n",
    "    total_arg_size = 0\n",
    "    shapes = [a.get_shape().as_list() for a in args]\n",
    "    for shape in shapes:\n",
    "        if len(shape) != 2:\n",
    "            raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n",
    "        if not shape[1]:\n",
    "            raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n",
    "        else:\n",
    "            total_arg_size += shape[1]\n",
    "    dtype = [a.dtype for a in args][0]\n",
    "\n",
    "    #sin = array_ops.concat(1, args)  batch_size* (x_dim + h_dim)\n",
    "    with vs.variable_scope(scope or \"Linear\"):\n",
    "#         matrix_x = vs.get_variable(\"Matrix_x\", [args[0].get_shape().as_list()[1], output_size])\n",
    "#         matrix_h = vs.get_variable(\"Matrix_h\", [args[1].get_shape().as_list()[1], output_size])\n",
    "#         res_x = math_ops.matmul(args[0], matrix_x) #tensornet.layers.tt(args[0], inp_modes['x'], out_modes['x'], mat_ranks['x'])\n",
    "#         res_h = math_ops.matmul(args[1], matrix_h)#tensornet.layers.tt(args[1], inp_modes['h'], out_modes['h'], mat_ranks['h'])\n",
    "#         res = res_x +  res_h #batch_size*out_size\n",
    "        res_x = tensornet.layers.mf_rnn(args[0], [shapes[0][1]], [output_size], mat_ranks['x'], scope =\"x\")\n",
    "        res_h = tensornet.layers.mf_rnn(args[1], [shapes[1][1]], [output_size], mat_ranks['h'], scope =\"h\")\n",
    "        res = res_x +  res_h\n",
    "        if not bias:\n",
    "            return res\n",
    "        bias_term = vs.get_variable(\"Bias\", [output_size],dtype=dtype,initializer=init_ops.constant_initializer(\n",
    "                bias_start, dtype=dtype))\n",
    "      \n",
    "    return res + bias_term"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
